{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is based on [this video on Youtube](https://www.youtube.com/watch?v=u1loyDCoGbE). However, in the concatination part, I guess there was a mistake and mine is different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture\n",
    "According to the [original paper](https://arxiv.org/pdf/1505.04597.pdf), U-Net has the following architecture:\n",
    "![U-net Architecture](Images/unet-architecture.png)  \n",
    "We can clearly see why it is called U-Net. I labeled some activation maps based on the code below.  \n",
    "As we see from the above architecture, each section has a double convolution. Hence, we define a function named `double_conv` in this code to simply does that.  \n",
    "**One of the important things** we also should consider is that when an **up-conv** happens (green arrows), sizes do not match; take the first one as an example, x9 before and after the up-conv is 1024x28x28 and 512x56x56, respectively. After the first up-conv, we want to concatenate this with x7 which is 512x64x64, and obviously 56 â‰  64. Therefore, this paper suggested to crop x7 to become 512x56x56. To do that, a function named `crop_image` is implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions described in the architecture section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_conv(in_c, out_c):\n",
    "    conv = nn.Sequential(\n",
    "        nn.Conv2d(in_c, out_c, kernel_size=3),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(out_c, out_c, kernel_size=3),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n",
    "    \n",
    "    return conv\n",
    "\n",
    "def crop_image(tensor, target_tensor):\n",
    "    target_size = target_tensor.size()[2]  # since height and width are the same we just get one of them\n",
    "    tensor_size = tensor.size()[2]\n",
    "    \n",
    "    delta = tensor_size - target_size\n",
    "    delta = delta // 2\n",
    "    return tensor[:, :, delta:tensor_size-delta, delta:tensor_size-delta]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing the U-Net "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "    \n",
    "        self.max_pool_2x2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.down_conv_1 = double_conv(in_c=1, out_c=64)\n",
    "        self.down_conv_2 = double_conv(in_c=64, out_c=128)\n",
    "        self.down_conv_3 = double_conv(in_c=128, out_c=256)\n",
    "        self.down_conv_4 = double_conv(in_c=256, out_c=512)\n",
    "        self.down_conv_5 = double_conv(in_c=512, out_c=1024)\n",
    "        \n",
    "        self.up_trans_1 = nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=2, stride=2)\n",
    "        self.up_conv_1 = double_conv(1024, 512)\n",
    "        \n",
    "        self.up_trans_2 = nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=2, stride=2)\n",
    "        self.up_conv_2 = double_conv(512, 256)\n",
    "        \n",
    "        self.up_trans_3 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=2, stride=2)\n",
    "        self.up_conv_3 = double_conv(256, 128)\n",
    "        \n",
    "        self.up_trans_4 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=2, stride=2)\n",
    "        self.up_conv_4 = double_conv(128, 64)\n",
    "        \n",
    "        self.out = nn.Conv2d(in_channels=64, out_channels=2, kernel_size=1)\n",
    "    \n",
    "    def forward(self, image):\n",
    "        # encoder\n",
    "        x1 = self.down_conv_1(image)\n",
    "        x2 = self.max_pool_2x2(x1)\n",
    "        x3 = self.down_conv_2(x2)\n",
    "        x4 = self.max_pool_2x2(x3)\n",
    "        x5 = self.down_conv_3(x4)\n",
    "        x6 = self.max_pool_2x2(x5)\n",
    "        x7 = self.down_conv_4(x6)\n",
    "        x8 = self.max_pool_2x2(x7)\n",
    "        x9 = self.down_conv_5(x8)\n",
    "        \n",
    "        # decoder\n",
    "        x = self.up_trans_1(x9)\n",
    "        y = crop_image(x7, x)\n",
    "        x= self.up_conv_1(torch.cat([y, x], 1))\n",
    "        \n",
    "        x = self.up_trans_2(x)\n",
    "        y = crop_image(x5, x)\n",
    "        x= self.up_conv_2(torch.cat([y, x], 1))\n",
    "        \n",
    "        x = self.up_trans_3(x)\n",
    "        y = crop_image(x3, x)\n",
    "        x= self.up_conv_3(torch.cat([y, x], 1))\n",
    "        \n",
    "        x = self.up_trans_4(x)\n",
    "        y = crop_image(x1, x)\n",
    "        x= self.up_conv_4(torch.cat([y, x], 1))\n",
    "        \n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 388, 388])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = torch.rand((1, 1, 572, 572))  # batch_size=1, channel=1, width & height = 572\n",
    "model = UNet()\n",
    "result = model(image)\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output size is the same as the one recommended in the paper, as can be seen. It has two channels for image segmentation, one for the foreground and the other for the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
